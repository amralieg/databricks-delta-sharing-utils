{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97679e89-9a4c-4aee-b10a-7a9d7e2073f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install delta-sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c7247eb-a2d4-414b-93d5-6c24c18ebc22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import delta_sharing\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import namedtuple\n",
    "import multiprocessing\n",
    "import threading \n",
    "import uuid\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "\n",
    "class DeltaShareRecipient:\n",
    "  \"\"\"\n",
    "  This class allows you to add a share files, and perform various operations that make it easy to work with delta share files.\n",
    "  \"\"\"\n",
    "  def __init__(self, share_profile_file_loc:str=\"\", provider_sharing_identifier:str=\"\", catalog:str=\"hive_metastore\", table_prefix:str=\"\"):\n",
    "    \"\"\"\n",
    "    Constructor method to initialize the class with the given parameters.\n",
    "\n",
    "    Args:\n",
    "        share_profile_file_loc (str): The path to the share file on dbfs or any other cloud storage location.\n",
    "        catalog (str, optional): The catalog to use for the tables. Defaults to \"hive_metastore\".\n",
    "        table_prefix (str, optional): The prefix to use for the tables. Defaults to \"\".\n",
    "    \"\"\"\n",
    "    self.__log(\"\", f\"DeltaShareRecipient started with share_profile_file_loc={share_profile_file_loc} and provider_sharing_identifier={provider_sharing_identifier}\")\n",
    "    if (share_profile_file_loc==\"\" and provider_sharing_identifier==\"\"):\n",
    "      raise Exception(\"You cannot use both share_profile_file_loc and provider_sharing_identifier, you can use either one of them.\")\n",
    "    elif share_profile_file_loc!=\"\" and provider_sharing_identifier!=\"\":\n",
    "      raise Exception(f\"You must provide value for either share_profile_file_loc={share_profile_file_loc} or provider_sharing_identifier={provider_sharing_identifier}, you cannot add values for both.\")\n",
    "    \n",
    "    self.share_profile_file_loc = share_profile_file_loc\n",
    "    self.provider_sharing_identifier = provider_sharing_identifier\n",
    "    self.table_prefix = table_prefix\n",
    "    self.catalog = catalog\n",
    "\n",
    "    if provider_sharing_identifier != \"\":\n",
    "      self.__log(\"\", \"Sharing type is Databricks to Databricks (D2D)\")\n",
    "      self.is_D2D_sharing = True # databricks-to-databricks\n",
    "      self.tables = list()\n",
    "      self.temp_catalog = f\"{self.catalog}_temp_\"\n",
    "    else:\n",
    "      self.__log(\"\", \"Sharing type is Databricks to Open (D2O)\")\n",
    "      self.is_D2D_sharing = False # databricks-to-open\n",
    "      self.temp_catalog = self.catalog\n",
    "      self.share_profile_file_loc = self.__load_share_profile(self.share_profile_file_loc)\n",
    "      self.deltasharing_client = delta_sharing.SharingClient(self.share_profile_file_loc.replace(\"dbfs:\", \"/dbfs/\"))\n",
    "      self.tables = self.deltasharing_client.list_all_tables()\n",
    "    \n",
    "    self.sync_runs_db = 'hive_metastore.default'\n",
    "    self.sync_runs_table = f\"{self.sync_runs_db}.delta_sharing_sync_runs\"\n",
    "    #get current user\n",
    "    self.current_user= self.__spark_sql(\"select current_user() as user;\").collect()[0][0]\n",
    "    self.__log(\"\", f\"delta sharing recipient initiated, shared tabled will be stored in catalog {catalog}\")\n",
    "    self.lock = threading.Lock()\n",
    "\n",
    "  def summerise(self, sync_ids:list, full_summary=False):\n",
    "    \"\"\"\n",
    "    This method will display the summary of the sync operations performed.\n",
    "\n",
    "    Args:\n",
    "        sync_ids (list): A list of sync ids.\n",
    "    \"\"\"\n",
    "    self.__log(\"\", \"starting summurisation of sync operation\")\n",
    "    syncs = (', '.join('\"' + sync_id + '\"' for sync_id in sync_ids))\n",
    "    if full_summary:\n",
    "      display(self.__spark_sql(f\"select * from {self.sync_runs_table} where sync_id in ({syncs}) order by completion_time desc;\"))\n",
    "    else:\n",
    "      display(self.__spark_sql(f\"select status, source_table, target_table, num_affected_rows,\\\n",
    "      (unix_timestamp(completion_time)-unix_timestamp(started_time)) as duration_seconds,\\\n",
    "      message from {self.sync_runs_table} where sync_id in ({syncs}) order by duration_seconds desc;\"))\n",
    "\n",
    "  def discover(self):\n",
    "    \"\"\"\n",
    "    Returns a dataframe with all the information about the share file, including share, schema, and table.\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: A dataframe containing the share, schema, and table.\n",
    "    \"\"\"\n",
    "    if self.is_D2D_sharing:\n",
    "      display(self.__spark_sql(f'SHOW SHARES IN PROVIDER `{self.provider_sharing_identifier}`;'))\n",
    "    else: \n",
    "      tables = self.deltasharing_client.list_all_tables()\n",
    "      if len(tables) > 0:\n",
    "        display(spark.createDataFrame(data=tables,\\\n",
    "           schema = [\"table\",\"schema\",\"share\"]).select(\"share\",\"schema\",\"table\"))\n",
    "      else:\n",
    "        self.__log(\"\", \"sharing file is empty. ask provider to add tables to this share to continue.\")\n",
    "  \n",
    "  def create_remotely_linked_tables(self, share:str)->list:\n",
    "    return self.__share_sync(share, cache_locally=False, refresh_incrementally=False,\\\n",
    "                 clear_previous_cache=True, clear_sync_history=False)\n",
    "\n",
    "  def create_fully_cached_tables(self, share:str)->list:\n",
    "    return self.__share_sync(share, cache_locally=True, refresh_incrementally=False,\\\n",
    "                 clear_previous_cache=True, clear_sync_history=False)\n",
    "  \n",
    "  def create_incrementally_cached_tables(self, share:str, primary_keys:dict)->list:\n",
    "    return self.__share_sync(share, cache_locally=True, refresh_incrementally=True,\\\n",
    "                 clear_previous_cache=False, clear_sync_history=False, primary_keys=primary_keys)\n",
    "  \n",
    "  def __share_sync(self, share:str=\"\", cache_locally:bool=False, refresh_incrementally:bool=False,\\\n",
    "                 clear_previous_cache:bool=False, clear_sync_history:bool=False, primary_keys:dict=dict(), num_threads=-1)->list:\n",
    "    \"\"\"\n",
    "    This method will sync all tables inside a aspecific share, if no share provided, it will sync all tables, from all shares.\n",
    "\n",
    "    Args:\n",
    "        cache_locally (bool, optional): Whether to cache the table locally. Defaults to False.\n",
    "        refresh_incrementally (bool, optional): Whether to refresh the cache incrementally, note CDF must be enabled on the source table. Defaults to False.\n",
    "        clear_previous_cache (bool, optional): Whether to clear the previous cache (warning: this will drop the tables and clear all content). Defaults to False.\n",
    "        clear_sync_history (bool, optional): Whether to clear the sync history. Defaults to False.\n",
    "        primary_keys (dict, optional): The primary keys for the the tables inside the share, this is needed for incremental updates to work. Defaults is empty {},\\\n",
    "        however you can pass it in this format {'table_x':'id1, id2, id3', 'table_y':'idx, idy'}.\n",
    "        num_threads (int, optional): how many concurrent threads to be used for sync\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sync ids.\n",
    "    \"\"\"\n",
    "    if num_threads == -1:\n",
    "      num_threads = multiprocessing.cpu_count()\n",
    "    try:\n",
    "      self.__log(\"\", f\"share sync started and will utilise {num_threads} threads of the assigned cluster driver node\")\n",
    "      sync_ids = list()\n",
    "      self.__clear_cache_and_sync_runs(cache_locally, refresh_incrementally, clear_previous_cache, clear_sync_history)\n",
    "\n",
    "      if self.is_D2D_sharing:\n",
    "        self.__create_d2d_tables(share, cache_locally)\n",
    "\n",
    "      with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = []\n",
    "        for table in self.tables:\n",
    "          if share==\"\" or (share!=\"\" and table.share == share):\n",
    "            target_schema = re.sub(r\"[^a-zA-Z0-9_]+\", \"_\", table.schema)\n",
    "            target_name = re.sub(r\"[^a-zA-Z0-9_]+\", \"_\", table.name)\n",
    "            future = executor.submit(self.table_sync, table.share, f\"`{table.schema}`.`{table.name}`\",\\\n",
    "                                    f\"{self.catalog}.{target_schema}.{self.table_prefix}{target_name}\",\\\n",
    "                                    primary_keys.get(f\"{table.schema}.{table.name}\"), cache_locally, refresh_incrementally, clear_previous_cache)\n",
    "            futures.append(future)\n",
    "        for future in futures:\n",
    "            sync_ids.append(future.result())\n",
    "      if len(sync_ids) > 0:\n",
    "        self.summerise(sync_ids)\n",
    "      else:\n",
    "        self.__log(\"\", \"sharing file is empty. ask provider to add tables to this share to continue.\")\n",
    "    finally:\n",
    "      if self.is_D2D_sharing:\n",
    "        self.__spark_sql(f'drop catalog if exists {self.temp_catalog} cascade;')\n",
    "        #pass\n",
    "    return sync_ids\n",
    "  \n",
    "  def table_sync(self, share:str, source:str, target:str, primary_keys:str=\"\", cache_locally:bool=False,\\\n",
    "                 refresh_incrementally:bool=False, clear_previous_cache:bool=False)->str:\n",
    "    \"\"\"\n",
    "    This method will sync a single table.\n",
    "\n",
    "    Args:\n",
    "        share (str): The share name.\n",
    "        source (str): The source schema and table name in this format 'schema.table'.\n",
    "        target (str): The target schema and table name to which you want to save the source to, in this format 'schema.table'.\n",
    "        primary_keys (dict, optional): The primary keys for the the tables inside the share, this is needed for incremental updates to work. Defaults is empty {},\\\n",
    "        however you can pass it in this format {'table_x':'id1, id2, id3', 'table_y':'idx, idy'}.\n",
    "        cache_locally (bool, optional): Whether to cache the table locally. Defaults to False.\n",
    "        refresh_incrementally (bool, optional): Whether to refresh the cache incrementally, note CDF must be enabled on the source table. Defaults to False.\n",
    "        clear_previous_cache (bool, optional): Whether to clear the previous cache (warning: this will drop the tables and clear all content). Defaults to False.\n",
    "    Returns:\n",
    "        str: The sync id.\n",
    "      \"\"\"\n",
    "    try:  \n",
    "      self.__log(source, f\"sync operation started for share={share} and source={source}\")\n",
    "      #, target={target}, cache_locally={cache_locally}, refresh_incrementally={refresh_incrementally}, clear_previous_cache={clear_previous_cache}\")\n",
    "      sync_id=None\n",
    "      if cache_locally == False:\n",
    "        sync_type = \"remotely-stored\"\n",
    "      elif refresh_incrementally == True:\n",
    "        sync_type = \"locally-cashed-incrementally-refreshed\"\n",
    "      else:\n",
    "        sync_type = \"locally-cashed-fully-refreshed\"\n",
    "      if cache_locally==False and refresh_incrementally:\n",
    "        raise Exception(\"Can not have refresh_incrementally=True while table is cache_locally=False, set refresh_incrementally=False and try again.\")\n",
    "\n",
    "      status = 'STARTED'\n",
    "      message = \"table_sync started\"\n",
    "      starting_from_version = -1\n",
    "      for table in self.tables:\n",
    "        if table.share == share and f\"`{table.schema}`.`{table.name}`\" == source:\n",
    "          self.__log(source, f\"database {self.catalog}.{table.schema} will be created if not exists.\")\n",
    "          self.__spark_sql(f\"CREATE DATABASE IF NOT EXISTS {self.catalog}.{table.schema};\")\n",
    "          break\n",
    "              \n",
    "      if clear_previous_cache:\n",
    "        if refresh_incrementally:\n",
    "          self.__log(\"\", \"warning: refresh_incrementally=True and clear_previous_cache=True, this will practically make incremental updates not effective, set clear_previous_cache=False to clear this warning.\")\n",
    "        #delete if it is D2O or D2D and cached locally\n",
    "\n",
    "        if (self.is_D2D_sharing == False) or (self.is_D2D_sharing and cache_locally):\n",
    "          self.__spark_sql(f\"DROP TABLE IF EXISTS {target};\")\n",
    "\n",
    "      #insert initial entry recrod to log this run where\n",
    "      sync_id=self.__log_table_sync_event(sync_id, share, source, starting_from_version, target, sync_type,\\\n",
    "                                          status, message,'null', 'null', 'null', 'null')\n",
    "      \n",
    "      temp_target = target.replace(self.catalog, self.temp_catalog)\n",
    "      self.__create_table(sync_id, share, source, target, temp_target, primary_keys, sync_type)\n",
    "    except Exception as e:\n",
    "      status = \"FAILED\"\n",
    "      message = \"table_sync failed, exception message: \" + str(e).replace('\\'','\"')\n",
    "      self.__log(source, message)\n",
    "      sync_id=self.__log_table_sync_event(sync_id, share, source, starting_from_version, target, sync_type,\\\n",
    "                                          status, message,'null', 'null', 'null', 'null')\n",
    "      #raise e;\n",
    "    return sync_id\n",
    "\n",
    "  def __clear_cache_and_sync_runs(self, cache_locally:bool=False, refresh_incrementally:bool=False,\\\n",
    "                                  clear_previous_cache:bool=False, clear_sync_history:bool=False):\n",
    "    self.__log(\"\", f\"cache managment started cache_locally={cache_locally}, refresh_incrementally={refresh_incrementally}, clear_sync_history={clear_sync_history}\")\n",
    "    \n",
    "    self.__spark_sql(f\"CREATE DATABASE IF NOT EXISTS {self.sync_runs_db};\")\n",
    "    \n",
    "    if clear_sync_history:\n",
    "      if refresh_incrementally:\n",
    "        self.__log(\"all\", \"warning: refresh_incrementally=True and clear_sync_history=True, this will practically make incremental updates not effective, set clear_sync_history=False to clear this warning.\")\n",
    "      self.__spark_sql(f\"DROP TABLE IF EXISTS {self.sync_runs_table};\")\n",
    "    \n",
    "    if self.is_D2D_sharing:\n",
    "      catalog_exists = False\n",
    "      self.__spark_sql(f\"drop catalog if exists {self.temp_catalog}\")\n",
    "      try:\n",
    "        catalog_type = self.__spark_sql(f'desc catalog {self.catalog}').where('info_name=\"Catalog Type\"').collect()[0][1]\n",
    "        if clear_previous_cache and catalog_type == \"Delta Sharing\":\n",
    "          self.__spark_sql(f\"drop catalog if exists {self.catalog} cascade;\")\n",
    "        else:\n",
    "          catalog_exists = True\n",
    "          self.__log(\"\", f\"catalog {self.catalog} already exists.\")\n",
    "      except Exception as e:\n",
    "        self.__log(\"\", f\"catalog {self.catalog} accepted to host shared data.\")\n",
    "      if catalog_exists and cache_locally == False:\n",
    "        raise Exception(f\"unable to create catalog {self.catalog} because it already exists. You must provide catalog name (other than hive_metastor) that does not exists to proceed. Alternatively, dorp this catalog and try again, you can use this command to drop the catalog: DROP CATALOG {self.catalog} cascade;\")\n",
    "\n",
    "\n",
    "  def __create_d2d_tables(self, share:str, cache_locally:bool):\n",
    "    if share == \"\":\n",
    "      raise Exception(\"You must specify a share if you are using a provider_sharing_identifier. call discover() to list all available shares and pick one that you would like to sync\")\n",
    "\n",
    "    catalog = self.temp_catalog\n",
    "    if cache_locally == False: #no need for temp catalog if we are going to create remotly-stored tables\n",
    "      catalog = self.catalog\n",
    "      if catalog == \"hive_metastore\":\n",
    "          raise Exception(\"Cannot use hive_metastore to host delta sharing table, provide another catalog name.\")\n",
    "      self.__log(\"\", f\"D2D sharing started, share {share} will be stored in catalog {catalog}\")\n",
    "    else:\n",
    "      self.__log(\"\", f\"D2D sharing started, share {share} will be stored in catalog {catalog} temporarily\")\n",
    "      self.__spark_sql(f'create catalog if not exists {self.catalog};')\n",
    "    self.__spark_sql(f'create catalog {catalog} using share `{self.provider_sharing_identifier}`.{share};')\n",
    "    databases = self.__get_database_objects(\"databases\", catalog, 'databaseName')\n",
    "    \n",
    "    SharedTable = namedtuple('Table', 'name schema share')\n",
    "    for database in databases:\n",
    "      if database == \"information_schema\":\n",
    "        continue\n",
    "      if cache_locally == True:\n",
    "        self.__spark_sql(f'create database if not exists {self.catalog}.{database};')\n",
    "      tables = self.__get_database_objects(\"tables\", f\"{catalog}.{database}\", 'tableName')\n",
    "      for table in tables:\n",
    "        self.tables.append(SharedTable(table, database, share))\n",
    "  \n",
    "  def __create_table(self, sync_id:str, share:str, source:str, target:str, temp_target:str, primary_keys:str, sync_type:str):\n",
    "    file_loc = self.share_profile_file_loc.replace(\"/dbfs\", \"dbfs:\")\n",
    "    table_url = f\"{file_loc}#{share}.{source}\".replace('`','') #must be in driver node fs\n",
    "    (last_sync_type, starting_from_version, last_completion_timestamp) = self.__get_last_sync_version(source, target)\n",
    "    max_table_version = self.__get_max_table_version(table_url, source)\n",
    "    self.__log(source, f\"table starting_From_version={starting_from_version} and max_table_version={max_table_version}\")\n",
    "    if sync_type == \"remotely-stored\":\n",
    "      self.__log(source, f\"table will be created remotely-stored\")\n",
    "      if self.is_D2D_sharing == False:#d2d tables are already created in the catalog, so no need to do anything here\n",
    "        if spark.catalog.tableExists(target) == False: \n",
    "          try:\n",
    "            self.__spark_sql(f\"\"\"CREATE TABLE {target}\n",
    "                        USING deltaSharing\n",
    "                        LOCATION '{table_url}';\"\"\")\n",
    "          except Exception as e:\n",
    "            if \"Unsupported cloud file system scheme `dbfs`\" in str(e):\n",
    "              raise Exception(f\"Unable to create table {target} on catalog {self.catalog}. possible reasons: \\n1- you are using a Unity Catalog enabled cluster. --> You are not allowed to create remote table for dbfs location on Unity Catalog cluster, change the provided catalog to hive_metastor to continue. or set cache_locally=True\")\n",
    "\n",
    "          status = \"SUCCESS\"\n",
    "          message = \"table created as remote table\"\n",
    "          num_affected_rows = \"null\"\n",
    "          num_updated_rows=\"null\"\n",
    "          num_deleted_rows=\"null\"\n",
    "          num_inserted_rows=\"null\"\n",
    "          starting_from_version = 'null'\n",
    "        else:\n",
    "          status = \"FAILED\"\n",
    "          message = f'Target table {target} already exists. drop it manually before continuing or set clear_previous_cache=True.\\\n",
    "          Note setting this option will drop this table \"{target}\" and clear all content'\n",
    "          num_affected_rows = \"null\"\n",
    "          num_updated_rows=\"null\"\n",
    "          num_deleted_rows=\"null\"\n",
    "          num_inserted_rows=\"null\"\n",
    "          starting_from_version = 'null'\n",
    "      else:\n",
    "        status = \"SUCCESS\"\n",
    "        message = \"table created as remote table\"\n",
    "        num_affected_rows = \"null\"\n",
    "        num_updated_rows=\"null\"\n",
    "        num_deleted_rows=\"null\"\n",
    "        num_inserted_rows=\"null\"\n",
    "        starting_from_version = 'null'\n",
    "    #create local copy if \n",
    "    # 1- sync_type = locally-cashed-fully-refreshed\n",
    "    # 2- sync_type = locally-cashed-incrementally-refreshedor but tables does not exists\n",
    "    # 3- sync_type = locally-cashed-incrementally-refreshedor but no previous sync version\n",
    "    # 4- sync_type = locally-cashed-incrementally-refreshedor and last sync version is greater than max table version (seems table is deleted and recreated at source)\n",
    "    # 5- sync_type = locally-cashed-incrementally-refreshedor and no primary keys provided\n",
    "    elif ((sync_type == \"locally-cashed-fully-refreshed\") or \n",
    "    (sync_type == \"locally-cashed-incrementally-refreshed\" and (self.is_D2D_sharing == False and spark.catalog.tableExists(f\"{target}\") == False)) or\n",
    "    (sync_type == \"locally-cashed-incrementally-refreshed\" and starting_from_version is None) or\n",
    "    (sync_type == \"locally-cashed-incrementally-refreshed\" and isinstance(max_table_version, str)) or\n",
    "    (sync_type == \"locally-cashed-incrementally-refreshed\" and starting_from_version > max_table_version) or\n",
    "    (sync_type == \"locally-cashed-incrementally-refreshed\" and (primary_keys is None or primary_keys.strip()==\"\"))):\n",
    "      self.__log(source, \"starting locally-cashed-fully-refreshed\")\n",
    "      if self.is_D2D_sharing:\n",
    "        spark.table(temp_target).write.saveAsTable(f\"{target}\", format=\"delta\", mode=\"overwrite\")\n",
    "      else:\n",
    "        delta_sharing.load_as_spark(table_url).write.saveAsTable(target, format=\"delta\", mode=\"overwrite\")\n",
    "      status = \"SUCCESS\"\n",
    "      message = \"table created as fully refreshed local table\"\n",
    "      if sync_type == \"locally-cashed-incrementally-refreshed\":\n",
    "        starting_from_version = max_table_version\n",
    "        message = f\"table was supposed to be refreshed incrementally, however, it failed and now it is created as full local cached copy. possible reasons for this 1- This is the first time to run the sync, 2- No primary keys specified to perform incremental updates. 3- table was dropped at source and recreated. 4- No previous recorded sync history for this table. 5- the {self.sync_runs_table} was deleted or cleared out.\"\n",
    "        self.__log(source, message)\n",
    "      else:\n",
    "        starting_from_version = 'null'\n",
    "      num_affected_rows = self.__get_table_count(target)\n",
    "      num_updated_rows=\"null\"\n",
    "      num_deleted_rows=\"null\"\n",
    "      num_inserted_rows=num_affected_rows\n",
    "    elif sync_type == \"locally-cashed-incrementally-refreshed\":\n",
    "      self.__log(source, \"starting locally-cashed-incrementally-refreshed\")\n",
    "      if starting_from_version == max_table_version:\n",
    "        self.__log(source, f\"table is up to date\")\n",
    "        status = \"SUCCESS\"\n",
    "        message = f\"update skipped, table {target} is up to date and last sync version is {starting_from_version}\"\n",
    "        self.__log(\"\", message)\n",
    "        num_affected_rows = 0\n",
    "        num_updated_rows=0\n",
    "        num_deleted_rows=0\n",
    "        num_inserted_rows=0\n",
    "      else:\n",
    "        if starting_from_version<=0 or starting_from_version > max_table_version:\n",
    "          self.__log(source, f\"table starting_from_version is not applicable, resetting to use max_table_version instead\")\n",
    "          starting_from_version = max_table_version\n",
    "        (status, message, starting_from_version, num_affected_rows, num_updated_rows, num_deleted_rows, num_inserted_rows) = \\\n",
    "        self.__sync_incrementally(sync_id, share, table_url, source, starting_from_version, max_table_version, target, temp_target,\\\n",
    "          primary_keys, last_sync_type, last_completion_timestamp)\n",
    "    else:\n",
    "      message = f\"Unknow sync type specified for table {source}\"\n",
    "      Exception(message)\n",
    "    self.__log_table_sync_event(sync_id, share, source, starting_from_version, target, sync_type, status,\\\n",
    "                             message, num_affected_rows, num_updated_rows, num_deleted_rows, num_inserted_rows)\n",
    "\n",
    "  def __sync_incrementally(self, sync_id:str, share:str, table_url, source:str, starting_from_version, max_table_version,\\\n",
    "                           target:str, temp_target:str, primary_keys:str, last_sync_type, last_completion_timestamp):  \n",
    "    \n",
    "    if primary_keys is None or primary_keys.strip()==\"\":\n",
    "      message = \"can not perform incremental refresh without specifying table primary keys. you can pass primary keys as dictionary,\\\n",
    "      for example {'table_x':'pk1, pk2', 'table_y':'pk3, pk4'}\"\n",
    "      self.__log(source, message)\n",
    "      raise Exception(message)\n",
    "    \n",
    "    conditions = [f\"__source.{c.strip()}=__target.{c.strip()}\" for c in primary_keys.split(',')]\n",
    "    on_conditions = \" and \".join([str(con) for con in conditions])\n",
    "    \n",
    "    view_name = re.sub(r\"[^a-zA-Z0-9_]+\", \"_\", source)+\"_temp_view\"\n",
    "    try:\n",
    "      if self.is_D2D_sharing:\n",
    "        self.__spark_sql(f'create or replace temp view {view_name} as select * from table_changes(\"{temp_target}\", {starting_from_version});')\n",
    "      else:\n",
    "        #create temp view with table changes\n",
    "        delta_sharing.load_table_changes_as_spark(url=table_url, starting_version=starting_from_version).createOrReplaceTempView(view_name)\n",
    "      \n",
    "      self.__spark_sql(f'create or replace temp view partitioned_{view_name} as \\\n",
    "      (select distinct * from (SELECT *, rank() over (partition by {primary_keys} order by _commit_version desc) as __rank FROM {view_name}) \\\n",
    "      where _change_type != \"update_preimage\" and __rank=1);')\n",
    "      \n",
    "      self.__log(source, \"starting merge operation from source to target\")\n",
    "      result = self.__spark_sql(f\"merge into {target} as __target using partitioned_{view_name} as __source on {on_conditions} \\\n",
    "      when matched and __source._change_type in ('insert', 'update_postimage') then update set *\\\n",
    "      when matched and __source._change_type in('delete','update_preimage') then delete\\\n",
    "      when not matched and __source._change_type in ('insert', 'update_postimage') then insert *;\")\\\n",
    "      .collect();\n",
    "      \n",
    "      num_affected_rows = result[0][0]\n",
    "      num_updated_rows = result[0][1]\n",
    "      num_deleted_rows = result[0][2]\n",
    "      num_inserted_rows = result[0][3]\n",
    "      latest_version = self.__spark_sql(f'select max(_commit_version) from {view_name};').collect()[0][0]\n",
    "      self.__log(source, f\"merge operation completed and resulted in {num_affected_rows} num_affected_rows\")\n",
    "      return (\"SUCCESS\", \"table incrementally updated successfully\", latest_version,\\\n",
    "              num_affected_rows, num_updated_rows, num_deleted_rows, num_inserted_rows)\n",
    "    finally:\n",
    "      self.__spark_sql(f'drop view if exists {view_name};')\n",
    "      self.__spark_sql(f'drop view if exists partitioned_{view_name};')\n",
    "  \n",
    "  def __get_last_sync_version(self, source:str, target:str):\n",
    "    last_version_df = self.__spark_sql(f\"select sync_type, source_last_sync_version, completion_time from {self.sync_runs_table}\\\n",
    "    where source_table='{source}' and status='SUCCESS' and source_last_sync_version = (select max(source_last_sync_version)\\\n",
    "    from {self.sync_runs_table} where source_table='{source}' and status='SUCCESS' and source_last_sync_version is not null)\\\n",
    "    order by completion_time desc limit 1;\")\n",
    "    self.__log(source, f\"table exists={spark.catalog.tableExists(target)}\")\n",
    "    if last_version_df.isEmpty() == False and spark.catalog.tableExists(target): #source must exists for the history to make sense\n",
    "      self.__log(source, f\"previous sync found\")\n",
    "      return (last_version_df.collect()[0][0], last_version_df.collect()[0][1], last_version_df.collect()[0][2])\n",
    "    else:\n",
    "      self.__log(source, f\"table has no usable sync history to use for incremental sync\")\n",
    "      return (None, None, None)\n",
    "  \n",
    "  def __get_max_table_version(self, table_url:str, source:str)->int:\n",
    "    try:\n",
    "      self.__log(source, f\"trying to get max_table_history\")\n",
    "      if self.is_D2D_sharing:\n",
    "        self.__spark_sql(f\"select * from table_changes('{self.temp_catalog}.{source}', {sys.maxsize}) limit 1;\").limit(1).collect()\n",
    "      else:\n",
    "        delta_sharing.load_table_changes_as_spark(url=table_url, starting_version=sys.maxsize).limit(1).collect()\n",
    "      version = sys.maxsize\n",
    "      self.__log(source, f\"table version is {version}\")\n",
    "    except Exception as e: #as expected the previous statement failed, lets extract the max table version from the error message\n",
    "      #self.__log(source, str(e))\n",
    "      pattern = r\"latest version of the table\\((\\d+)\\)\"\n",
    "      match = re.search(pattern, str(e))\n",
    "      if match:\n",
    "        version = int(match.group(1))\n",
    "      else:\n",
    "        version = 'null'\n",
    "    return version\n",
    "  \n",
    "  def __log_table_sync_event(self, sync_id:str, share:str, source:str, starting_from_version:int, target:str, sync_type:str,\\\n",
    "                             status:str, message:str, num_affected_rows, num_updated_rows,num_deleted_rows, num_inserted_rows):\n",
    "    with self.lock:\n",
    "      if spark.catalog.tableExists(self.sync_runs_table) == False:\n",
    "        #create sync_log table if not exists\n",
    "        self.__spark_sql(f\"create table {self.sync_runs_table} (sync_id string, share string, source_table string, source_last_sync_version int,\\\n",
    "        target_table string, started_by string, started_time timestamp, completion_time timestamp, status string, sync_type string,\\\n",
    "        num_affected_rows int, num_updated_rows int, num_deleted_rows int, num_inserted_rows int, message string);\")\n",
    "      if sync_id is None:\n",
    "        sync_id = str(uuid.uuid4())\n",
    "        self.__spark_sql(f\"\"\"insert into {self.sync_runs_table} values('{sync_id}', '{share}', '{source}', null, '{target}', '{self.current_user}',\\\n",
    "        current_timestamp(), null, '{status}', '{sync_type}', null, null, null, null, '{message}');\"\"\")\n",
    "      else:\n",
    "        self.__spark_sql(f\"update {self.sync_runs_table} set source_last_sync_version={starting_from_version},\\\n",
    "        completion_time = current_timestamp(), status='{status}', sync_type='{sync_type}', num_affected_rows={num_affected_rows},\\\n",
    "        num_updated_rows = {num_updated_rows}, num_deleted_rows = {num_deleted_rows}, num_inserted_rows={num_inserted_rows},\\\n",
    "        message='{message}' where sync_id='{sync_id}';\") \n",
    "    return sync_id\n",
    "\n",
    "  def __get_table_count(self, target):\n",
    "      return self.__spark_sql(f\"select count(*) from {target};\").collect()[0][0]\n",
    "\n",
    "  def __spark_sql(self, sql):\n",
    "    #print(sql)\n",
    "    #print()\n",
    "    return spark.sql(sql)\n",
    "\n",
    "  def __log(self, id, msg):\n",
    "    if id == \"\":\n",
    "      print(f\"[info] {msg}\")\n",
    "    else:\n",
    "      print(f\"[info - {id}] {msg}\")\n",
    "    pass\n",
    "  \n",
    "  def __get_database_objects(self, object_type, source, selector):\n",
    "    return list(self.__spark_sql(f\"show {object_type} in {source};\").toPandas()[selector])\n",
    "\n",
    "  def __load_share_profile(self, share_profile_loc:str):\n",
    "    # Check if the input starts with http\n",
    "    file_name = share_profile_loc\n",
    "    if share_profile_loc.startswith(\"http\"):\n",
    "      # Generate the file name\n",
    "      directory_path = \"/dbfs/deltasharing/profiles/\"\n",
    "      file_name = directory_path + re.sub(r'[^a-zA-Z0-9_]', '', share_profile_loc.replace(\".share\", \"\")) + \".share\"\n",
    "      try:\n",
    "          url = share_profile_loc\n",
    "          self.__log(\"\", \"loading share profile form online location\")\n",
    "          if 'databricks' in share_profile_loc and \"retrieve_config.html\" in share_profile_loc:\n",
    "            url = share_profile_loc.replace(\"/delta_sharing/retrieve_config.html?\", \"/api/2.0/unity-catalog/public/data_sharing_activation/\")\n",
    "            self.__log(\"\", f\"url is now pointing at {url}\")\n",
    "          \n",
    "          # Read the file from the URL and store its content as a string\n",
    "          content = requests.get(url).text\n",
    "          # Check if the content contains 'shareCredentialsVersion'\n",
    "          if 'shareCredentialsVersion' in content:\n",
    "              # Create directories if they don't exist\n",
    "              if not os.path.exists(directory_path):\n",
    "                  os.makedirs(directory_path)\n",
    "              with open(file_name, 'w') as file:\n",
    "                  file.write(content)\n",
    "                  self.__log(\"\", f\"file saved successfully to {file_name}\")\n",
    "          else:\n",
    "              print(f\"file can be read form online location, however it is not a valid delta share profile, will try loading form local cache.\")\n",
    "      except Exception as e:\n",
    "        pass\n",
    "      \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(file_name):\n",
    "        raise Exception(f\"Share profile {share_profile_loc} cannot be loaded from this location. if this an online location, try downloading the file manually, save it to dbfs and try again after setting share_profile_file_loc to the dbfs location of the saved file.\")\n",
    "    else:\n",
    "      self.__log(\"\", f\"file is available at {file_name}\")\n",
    "      return file_name\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 75528426337488,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "data_recipient",
   "notebookOrigID": 2915748167760417,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
