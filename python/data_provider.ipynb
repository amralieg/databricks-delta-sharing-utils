{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbfb2a82-3f5d-437f-8058-aef866ced769",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DeltaShareProvider:\n",
    "  def __init__(self, share:str, recipient:str, recipient_sharing_identifier:str=\"\", drop_share_if_exists:bool=False, drop_recipient_if_exists:bool=False):\n",
    "    \"\"\"\n",
    "    Initializes a DeltaShareProvider instance with the given parameters.\n",
    "\n",
    "    Args:\n",
    "        share (str): The name of the Delta share to use.\n",
    "        recipient (str): The name of the recipient to share the Delta share with.\n",
    "        recipient_sharing_identifier (str): The Databricks ID of the recipient. Defaults to an empty string.\n",
    "        drop_if_exists (bool): Whether to drop the recipient and the share if they already exist. Defaults to False.\n",
    "    \"\"\"\n",
    "    self.share = share\n",
    "    if drop_share_if_exists:\n",
    "      self.drop_share()\n",
    "    if drop_recipient_if_exists:\n",
    "      self.drop_recipient(recipient)\n",
    "    self.__spark_sql(f\"CREATE SHARE IF NOT EXISTS {share};\")\n",
    "    self.add_recipient(recipient, recipient_sharing_identifier)\n",
    "    \n",
    "  def drop_share(self):\n",
    "    \"\"\"\n",
    "    Drops the Delta share if it exists.\n",
    "    \"\"\"\n",
    "    self.__log(f\"share {self.share} will be dropped.\")\n",
    "    self.__spark_sql(f\"DROP SHARE IF EXISTS {self.share};\")\n",
    "  \n",
    "  def share_catalog(self, catalog:str, enable_cdf:bool=False):\n",
    "    \"\"\"\n",
    "    Shares all databases in the specified catalog to the Delta share.\n",
    "\n",
    "    Args:\n",
    "        catalog (str): The name of the catalog to share.\n",
    "        enable_cdf (bool): Whether to enable change data feed (CDF) on the shared tables. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DeltaShareProvider: The instance of DeltaShareProvider.\n",
    "    \"\"\"\n",
    "    databases = self.__get_database_objects(\"databases\", catalog, 'databaseName')\n",
    "    self.__log(f'sharing all databases in catalog {catalog} to share {self.share}')\n",
    "    for database in databases:\n",
    "      if database == \"information_schema\" or databases == \"default\":\n",
    "        self.__log(f\"skipping sharing database {catalog}.{database}\")\n",
    "        continue\n",
    "      self.share_database(f\"{catalog}.{database}\", enable_cdf)\n",
    "    self.__log(f'all databases in catalog {catalog} shared in share {self.share}')\n",
    "    return self\n",
    "  \n",
    "  def unshare_catalog(self, catalog:str):\n",
    "    \"\"\"\n",
    "    Removes all databases in the specified catalog from the Delta share.\n",
    "\n",
    "    Args:\n",
    "        catalog (str): The name of the catalog to remove from the Delta share.\n",
    "\n",
    "    Returns:\n",
    "        DeltaShareProvider: The instance of DeltaShareProvider.\n",
    "    \"\"\"\n",
    "    databases = self.__get_database_objects(\"databases\", catalog, 'databaseName')\n",
    "    self.__log(f'unsharing all databases in catalog {catalog} from share {self.share}')\n",
    "    for database in databases:\n",
    "      if database == \"information_schema\" or databases == \"default\":\n",
    "        continue\n",
    "      self.unshare_database(f\"{catalog}.{database}\")\n",
    "    self.__log(f'catalog {catalog} compeletly unshared from share {self.share}')\n",
    "    return self\n",
    "  \n",
    "  def share_database(self, database:str, enable_cdf:bool=False):\n",
    "    \"\"\"\n",
    "    Shares all tables in the specified database to the Delta share.\n",
    "\n",
    "    Args:\n",
    "        database (str): The name of the database to share.\n",
    "        enable_cdf (bool): Whether to enable change data feed (CDF) on the shared tables. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        DeltaShareProvider: The instance of DeltaShareProvider.\n",
    "    \"\"\"\n",
    "    tables = self.__get_database_objects(\"tables\", database, 'tableName')\n",
    "    self.__log(f'sharing all tables in database {database} to share {self.share}')\n",
    "    for table in tables:\n",
    "      self.share_table(f\"{database}.{table}\", enable_cdf)\n",
    "    self.__log(f'all tables in database {database} shared in share {self.share}')\n",
    "    return self\n",
    "  \n",
    "  def unshare_database(self, database:str):\n",
    "    \"\"\"\n",
    "    Unshares all tables in the specified database from the share.\n",
    "    \n",
    "    Args:\n",
    "        database (str): Name of the database.\n",
    "    \n",
    "    Returns:\n",
    "        self\n",
    "    \"\"\"\n",
    "    tables = self.__get_database_objects(\"tables\", database, 'tableName')\n",
    "    self.__log(f'unsharing all tables in database {database} from share {self.share}')\n",
    "    for table in tables:\n",
    "      self.unshare_table(f\"{database}.{table}\")\n",
    "    self.__log(f'database {database} compeletly unshared from share {self.share}')\n",
    "    return self\n",
    "  \n",
    "  def share_table(self, table:str, enable_cdf:bool=False):\n",
    "    \"\"\"\n",
    "    Shares the specified table to the share.\n",
    "    \n",
    "    Args:\n",
    "        table (str): Name of the table.\n",
    "        enable_cdf (bool): Whether to enable Change Data Feed for the shared table (default False).\n",
    "    \n",
    "    Returns:\n",
    "        self\n",
    "    \"\"\"\n",
    "    try:\n",
    "      if enable_cdf:\n",
    "        self.unshare_table(table) #unshare it first to enable cdf\n",
    "        self.__log(f\"enabling CDF on table {table}, this might take few minutes to complete.\")\n",
    "        self.__spark_sql(f'ALTER TABLE {table} SET TBLPROPERTIES (delta.enableChangeDataFeed = true);')\n",
    "        self.__spark_sql(f'ALTER SHARE {self.share} ADD TABLE {table} WITH HISTORY;')\n",
    "        self.__log(f'table {table} added to share {self.share} with CDF and History turned on')\n",
    "      else:\n",
    "        self.__spark_sql(f'ALTER SHARE {self.share} ADD TABLE {table};')\n",
    "        self.__log(f'table {table} added to share {self.share}')\n",
    "    except Exception as e:\n",
    "      self.__log(\"exception occured, message is: \" + str(e))\n",
    "    return self\n",
    "  \n",
    "  def unshare_table(self, table:str):\n",
    "    \"\"\"\n",
    "    Unshares the specified table from the share.\n",
    "    \n",
    "    Args:\n",
    "        table (str): Name of the table.\n",
    "    \n",
    "    Returns:\n",
    "        self\n",
    "    \"\"\"\n",
    "    try:\n",
    "      self.__log(f\"will try to remove table {table} form share {self.share} first.\")\n",
    "      self.__spark_sql(f'ALTER SHARE {self.share} REMOVE TABLE {table};')\n",
    "    except Exception as e:\n",
    "      pass #self.__log(\"exception occured, message is: \" + str(e))\n",
    "    return self\n",
    "  \n",
    "  def add_recipient(self, recipient:str, recipient_sharing_identifier:str=\"\"):\n",
    "    \"\"\"\n",
    "    Adds a recipient to the share and grants SELECT access to them.\n",
    "    \n",
    "    Args:\n",
    "        recipient (str): Name of the recipient.\n",
    "        recipient_sharing_identifier (str): ID of the Databricks instance where the recipient is located (optional).\n",
    "    \n",
    "    Returns:\n",
    "        self\n",
    "    \"\"\"\n",
    "    if recipient_sharing_identifier is None or recipient_sharing_identifier.strip()==\"\":\n",
    "      self.__log(f'recipient {recipient} will be created. copy the code that will show after completion and share it with the recipients.')\n",
    "      result = self.__spark_sql(f'create recipient if not exists {recipient};').select(\"info_value\").where(\"info_name='activation_link'\").collect()\n",
    "      if len(result) > 0:\n",
    "        activation_link = result[0][0]\n",
    "      else:\n",
    "        activation_link = \"<share_profile_file_location>\"\n",
    "      display(self.__spark_sql(f\"select 'DeltaShareRecipient(share_profile_file_loc=\\\"{activation_link}\\\", catalog=\\\"hive_metastore\\\").create_remotely_linked_tables(share=\\\"{self.share}\\\")' as run_this_code_at_recipient\"))\n",
    "    else:\n",
    "      existing = self.__find_recipient_by_sharing_id(recipient_sharing_identifier)\n",
    "      if existing != None and existing != recipient:\n",
    "        raise Exception(f\"cannot add recipient {recipient} because another recipient {existing} alread defined using the same recipient_shareing_identifier={recipient_sharing_identifier}. re-run again, however, use {existing} as recipient instead of {recipient}. Alternatively, drop {existing} first so it can be created.\")\n",
    "      display(self.__spark_sql(f'create recipient if not exists {recipient} using ID \"{recipient_sharing_identifier}\";'))\n",
    "      self.__log(f'databirkcs recipient {recipient} created using the sharing identifier provided. inform the recipient so they can start reading the shares.')\n",
    "      self.__spark_sql(\"select concat('DeltaShareRecipient(provider_shareing_identifier=\\\"', current_metastore(), '\\\", catalog=\\\"hive_metastore\\\").create_remotely_linked_tables(share=\\\"', '{self.share}', '\\\")') as run_this_code_at_recipient\")\n",
    "\n",
    "    self.__spark_sql(f'GRANT SELECT on SHARE {self.share} TO RECIPIENT {recipient};')\n",
    "    self.__log(f'recipient {recipient} granted SELECT on share {self.share}')\n",
    "    return self\n",
    "  \n",
    "  def __find_recipient_by_sharing_id(self, recipient_sharing_identifier:str)->str:\n",
    "    recipients = list(self.__spark_sql(\"show recipients;\").where(\"authentication_type='DATABRICKS'\").select(\"recipient\").toPandas()['recipient'])\n",
    "    recipient_name = None\n",
    "    for recipient in recipients:\n",
    "      df = self.__spark_sql(f\"desc recipient {recipient};\")\n",
    "      cloud = \"\"\n",
    "      region=\"\"\n",
    "      metastore_id = \"\"\n",
    "      for row in df.collect():\n",
    "        if row.info_name == \"recipient_name\":\n",
    "          recipient_name = row.info_value\n",
    "        if row.info_name == \"cloud\":\n",
    "          cloud = row.info_value\n",
    "        if row.info_name == \"region\":\n",
    "          region = row.info_value\n",
    "        if row.info_name == \"metastore_id\":\n",
    "          metastore_id = row.info_value\n",
    "      if f\"{cloud}:{region}:{metastore_id}\" == recipient_sharing_identifier:\n",
    "        break\n",
    "    return recipient_name\n",
    "  \n",
    "  def remove_recipient(self, recipient:str):\n",
    "    \"\"\"\n",
    "    Removes SELECT access to the specified recipient from the share.\n",
    "    \n",
    "    Args:\n",
    "        recipient (str): Name of the recipient.\n",
    "    \n",
    "    Returns:\n",
    "        self\n",
    "    \"\"\"\n",
    "    try:\n",
    "      self.__spark_sql(f'REVOKE SELECT ON SHARE {self.share} FROM RECIPIENT {recipient};')\n",
    "      self.__log(f'SELECT access on share {self.share} is revoked from  recipient {recipient}')\n",
    "    except Exception as e:\n",
    "      self.__log(\"exception occured, message is: \" + str(e))\n",
    "    return self\n",
    "  \n",
    "  def drop_recipient(self, recipient:str):\n",
    "    \"\"\"\n",
    "    Drops the specified recipient.\n",
    "    \n",
    "    Args:\n",
    "        recipient (str): Name of the recipient.\n",
    "    \n",
    "    Returns:\n",
    "        self\n",
    "    \"\"\"\n",
    "    try:\n",
    "      self.__log(f\"recipient {recipient} will be dropped.\")\n",
    "      self.__spark_sql(f'DROP RECIPIENT IF EXISTS {recipient};')\n",
    "    except Exception as e:\n",
    "      self.__log(\"exception occured, message is: \" + str(e))\n",
    "    return self\n",
    "    \n",
    "  def __spark_sql(self, sql):\n",
    "    print(sql)\n",
    "    print()\n",
    "    return spark.sql(sql)\n",
    "\n",
    "  def __log(self, thing):\n",
    "    print(\"[info] \" + thing)\n",
    "    pass\n",
    "  \n",
    "  def __get_database_objects(self, object_type, source, selector):\n",
    "    return list(self.__spark_sql(f\"show {object_type} in {source};\").toPandas()[selector])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "data_provider",
   "notebookOrigID": 2915748167760420,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
