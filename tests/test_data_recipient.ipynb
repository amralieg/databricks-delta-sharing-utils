{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe70438-0191-4dda-adde-cf4150d96130",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../python/data_recipient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "814d5658-2a03-4f15-974d-0ec6b6ebc780",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def test_data_recipient(share, share_file, sharing_identifier, catalog, primary_keys={}):\n",
    "  \n",
    "  def create_recipient(share_file, sharing_identifier, catalog):\n",
    "    if share_file != \"\":\n",
    "      return DeltaShareRecipient(share_profile_file_loc=share_file, catalog=catalog)\n",
    "    elif sharing_identifier != \"\":\n",
    "      return DeltaShareRecipient(share_profile_file_loc=share_file, provider_sharing_identifier=sharing_identifier, catalog=catalog)\n",
    "    else:\n",
    "      raise Exception()\n",
    "  \n",
    "  def test_discover():\n",
    "    dsr = create_recipient(share_file, sharing_identifier, catalog)\n",
    "    dsr.discover()\n",
    "\n",
    "  def test_sync_remotely():\n",
    "    dsr = create_recipient(share_file, sharing_identifier, catalog)\n",
    "    dsr.create_remotely_linked_tables(share)\n",
    "\n",
    "  def test_sync_remotely_hms():\n",
    "    if sharing_identifier == \"\":\n",
    "      dsr = create_recipient(share_file, sharing_identifier, \"hive_metastore\")\n",
    "    else:\n",
    "      spark.sql(f\"drop catalog if exists amrali_d2d cascade;\")\n",
    "      dsr = create_recipient(share_file, sharing_identifier, catalog)\n",
    "    dsr.create_remotely_linked_tables(share)\n",
    "\n",
    "  def test_sync_full():\n",
    "    dsr = create_recipient(share_file, sharing_identifier, catalog)\n",
    "    dsr.create_fully_cached_tables(share)\n",
    "\n",
    "  def test_sync_incremental():\n",
    "    dsr = create_recipient(share_file, sharing_identifier, catalog)\n",
    "    dsr.create_incrementally_cached_tables(share, primary_keys)\n",
    "  \n",
    "  spark.sql(f\"drop catalog if exists amrali_d2d cascade;\")\n",
    "  if sharing_identifier ==\"\":\n",
    "    spark.sql(f\"create catalog if not exists {catalog}\")\n",
    "  test_discover()\n",
    "  test_sync_remotely()\n",
    "  test_sync_remotely_hms()\n",
    "  test_sync_full()\n",
    "  test_sync_incremental()\n",
    "  print(\"------------------------------------------------------\")\n",
    "  print(\"--\")\n",
    "  print(\"-- Run some data changes on the source before running incremental updates again\")\n",
    "  print(\"--\")\n",
    "  print(\"------------------------------------------------------\")\n",
    "  time.sleep(60) #allow sometime to run some data changes\n",
    "  test_sync_incremental()\n",
    "\n",
    "#test_data_recipient(share=\"amr_share\", share_file = '/dbfs/FileStore/tables/amr_azure_share.share',\\\n",
    "#                    sharing_identifier=\"\", catalog=\"amrali_d2o\", primary_keys = {'db1.table1':'id', 'db1.table2':'idx'})\n",
    "test_data_recipient(share=\"amr_share\", share_file=\"\", sharing_identifier='<>',\\\n",
    "                    catalog=\"amrali_d2d\", primary_keys = {'db1.table1':'id', 'db1.table2':'idx'})\n",
    "\n",
    "#test_data_recipient(share_file = '/dbfs/FileStore/tables/open_datasets.share', catalog=\"amrali_d2o\", primary_keys = {'db1.table1':'id', 'db1.table2':'idx'})"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "test_data_recipient",
   "notebookOrigID": 597306790366617,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
