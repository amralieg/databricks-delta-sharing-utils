{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfe70438-0191-4dda-adde-cf4150d96130",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../python/data_provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "814d5658-2a03-4f15-974d-0ec6b6ebc780",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def test_data_provider(share, share_file, sharing_identifier, catalog):\n",
    "  \n",
    "  def create_recipient(share_file, sharing_identifier, catalog):\n",
    "    if share_file != \"\":\n",
    "      return DeltaShareRecipient(share_file_loc=share_file, catalog=catalog)\n",
    "    elif sharing_identifier != \"\":\n",
    "      return DeltaShareRecipient(share_file, provider_sharing_identifier=sharing_identifier, catalog=catalog)\n",
    "    else:\n",
    "      raise Exception()\n",
    "  \n",
    "  def test_discover():\n",
    "    dsr = create_recipient(share_file, sharing_identifier, catalog)\n",
    "    dsr.discover()\n",
    "\n",
    "  def test_sync_remotely():\n",
    "    dsr = create_recipient(share_file, sharing_identifier, catalog)\n",
    "    dsr.share_sync(share, cache_locally=False, refresh_incrementally=False, clear_previous_cache=True, clear_sync_history=False,\\\n",
    "                  primary_keys = {'db1.table1':'id', 'db1.table2':'idx'}, num_threads=3)\n",
    "\n",
    "  def test_sync_remotely_hms():\n",
    "    if sharing_identifier == \"\":\n",
    "      dsr = create_recipient(share_file, sharing_identifier, \"hive_metastore\")\n",
    "    else:\n",
    "      dsr = create_recipient(share_file, sharing_identifier, catalog)\n",
    "    dsr.share_sync(share, cache_locally=False, refresh_incrementally=False, clear_previous_cache=True, clear_sync_history=False,\\\n",
    "                  primary_keys = {'db1.table1':'id', 'db1.table2':'idx'}, num_threads=3)\n",
    "\n",
    "  def test_sync_full():\n",
    "    dsr = create_recipient(share_file, sharing_identifier, catalog)\n",
    "    dsr.share_sync(share, cache_locally=True, refresh_incrementally=False, clear_previous_cache=True, clear_sync_history=False,\\\n",
    "                  primary_keys = {'db1.table1':'id', 'db1.table2':'idx'}, num_threads=3)\n",
    "\n",
    "  def test_sync_full_clear():\n",
    "    dsr = create_recipient(share_file, sharing_identifier, catalog)\n",
    "    dsr.share_sync(share, cache_locally=True, refresh_incrementally=False, clear_previous_cache=True, clear_sync_history=False,\\\n",
    "                  primary_keys = {'db1.table1':'id', 'db1.table2':'idx'}, num_threads=3)\n",
    "\n",
    "  def test_sync_incremental():\n",
    "    dsr = create_recipient(share_file, sharing_identifier, catalog)\n",
    "    dsr.share_sync(share, cache_locally=True, refresh_incrementally=True, clear_previous_cache=False, clear_sync_history=False,\\\n",
    "                  primary_keys = {'db1.table1':'id', 'db1.table2':'idx'}, num_threads=3)\n",
    "  \n",
    "  spark.sql(f\"drop catalog if exists amrali_d2d cascade;\")\n",
    "  if sharing_identifier ==\"\":\n",
    "    spark.sql(f\"create catalog if not exists {catalog}\")\n",
    "  test_discover()\n",
    "  test_sync_remotely()\n",
    "  test_sync_remotely_hms()\n",
    "  test_sync_full()\n",
    "  test_sync_incremental()\n",
    "  print(\"------------------------------------------------------\")\n",
    "  print(\"--\")\n",
    "  print(\"-- Run some data changes on the source before running incremental updates again\")\n",
    "  print(\"--\")\n",
    "  print(\"------------------------------------------------------\")\n",
    "  time.sleep(60) #allow sometime to run some data changes\n",
    "  test_sync_incremental()\n",
    "\n",
    "#test_data_provider(share=\"amr_share\", share_file = '/dbfs/FileStore/tables/amr_azure_share.share', sharing_identifier=\"\", catalog=\"amrali_d2o\")\n",
    "test_data_provider(share=\"amr_share\", share_file=\"\", sharing_identifier='azure:eastus2:b86c6879-8c55-4e70-a585-18d16a4fa6e9', catalog=\"amrali_d2d\")\n",
    "\n",
    "#test_data_provider(share_file = '/dbfs/FileStore/tables/open_datasets.share', catalog=\"amrali_d2o\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "test_data_provider",
   "notebookOrigID": 597306790366633,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
